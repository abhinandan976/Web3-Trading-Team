{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8NdADG-QrNkW",
        "outputId": "62b3ddbb-d697-40f2-b709-505a7882b7ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting Notebook 1: Data Preparation ---\n",
            "Data loaded successfully: Trader Records (211224), Sentiment Records (2644)\n",
            "\n",
            "[STEP 2.1] Cleaning Trader Data...\n",
            "  > WARNING: Missing column 'leverage'. Created a default proxy column (1.0).\n",
            "[STEP 2.2] Cleaning Sentiment Data...\n",
            "  > Cleaned Sentiment Data unique days: 2644\n",
            "\n",
            "[STEP 3] Feature Engineering and Daily Aggregation...\n",
            "  > Aggregated metrics created: 7 days.\n",
            "\n",
            "[STEP 4] Merging Trader Metrics with Market Sentiment...\n",
            "  > Final Merged Dataset size: 6 records (days with both data sources).\n",
            "  > Data range: 2023-03-28 to 2025-02-19\n",
            "\n",
            "[STEP 5] Saving Intermediate Output...\n",
            "SUCCESS: Intermediate merged data saved to: csv_files/daily_metrics_merged.csv\n",
            "--- Notebook 1 Completed. Proceed to Notebook 2 for Analysis. ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4140778641.py:40: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df_trader[col].fillna(0, inplace=True)\n",
            "/tmp/ipython-input-4140778641.py:88: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  daily_trader_metrics['StdDev_Daily_PnL'].fillna(0, inplace=True)\n"
          ]
        }
      ],
      "source": [
        "# This notebook is designed for professional data ingestion, cleaning, and feature engineering.\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# --- 1. SETUP AND FILE LOADING ---\n",
        "print(\"--- Starting Notebook 1: Data Preparation ---\")\n",
        "TRADER_FILE = '/content/historical_data.csv'\n",
        "SENTIMENT_FILE = '/content/fear_greed_index.csv'\n",
        "OUTPUT_DIR = 'csv_files'\n",
        "OUTPUT_FILE_PATH = os.path.join(OUTPUT_DIR, 'daily_metrics_merged.csv')\n",
        "\n",
        "# Create the required output directory for the standardized submission format\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "try:\n",
        "    df_trader = pd.read_csv(TRADER_FILE, low_memory=False)\n",
        "    df_sentiment = pd.read_csv(SENTIMENT_FILE)\n",
        "    print(f\"Data loaded successfully: Trader Records ({len(df_trader)}), Sentiment Records ({len(df_sentiment)})\")\n",
        "except Exception as e:\n",
        "    print(f\"CRITICAL ERROR: Failed to load data. Ensure '{TRADER_FILE}' and '{SENTIMENT_FILE}' are available. Error: {e}\")\n",
        "    exit()\n",
        "\n",
        "# --- 2. DATA CLEANING AND PREPROCESSING ---\n",
        "\n",
        "# 2.1. Clean Trader Data (Historical Data)\n",
        "print(\"\\n[STEP 2.1] Cleaning Trader Data...\")\n",
        "\n",
        "# Convert Unix timestamp (high precision in ms or similar) to a simple date object\n",
        "df_trader['Date'] = pd.to_datetime(df_trader['Timestamp'], unit='ms').dt.normalize().dt.date\n",
        "\n",
        "# Standardize column names for easier access and check for PnL/Volume\n",
        "# Note: The assignment specifies 'closedPnL' and 'size', but actual columns are 'Closed PnL' and 'Size USD'.\n",
        "numeric_cols = ['Closed PnL', 'Size USD']\n",
        "for col in numeric_cols:\n",
        "    df_trader[col] = pd.to_numeric(df_trader[col], errors='coerce')\n",
        "    # Fill any NaNs created by coercion (usually none, but good practice)\n",
        "    df_trader[col].fillna(0, inplace=True)\n",
        "\n",
        "# --- CRITICAL HANDLING: Missing 'leverage' column ---\n",
        "LEVERAGE_COL = 'leverage'\n",
        "if LEVERAGE_COL not in df_trader.columns:\n",
        "    # As true leverage cannot be calculated without account equity, we use a proxy.\n",
        "    # We choose a neutral value (1.0) and use PnL/Volume volatility as a risk proxy later.\n",
        "    df_trader[LEVERAGE_COL] = 1.0\n",
        "    print(f\"  > WARNING: Missing column '{LEVERAGE_COL}'. Created a default proxy column (1.0).\")\n",
        "\n",
        "\n",
        "# 2.2. Clean Sentiment Data\n",
        "print(\"[STEP 2.2] Cleaning Sentiment Data...\")\n",
        "\n",
        "# Rename and convert the date column for merging\n",
        "df_sentiment.rename(columns={'classification': 'Market_Sentiment'}, inplace=True)\n",
        "df_sentiment['Date'] = pd.to_datetime(df_sentiment['date']).dt.date\n",
        "# Drop duplicates (if any day has multiple entries) and keep the last (most recent)\n",
        "df_sentiment = df_sentiment[['Date', 'Market_Sentiment']].drop_duplicates(subset=['Date'], keep='last')\n",
        "print(f\"  > Cleaned Sentiment Data unique days: {len(df_sentiment)}\")\n",
        "\n",
        "\n",
        "# --- 3. FEATURE ENGINEERING & AGGREGATION ---\n",
        "print(\"\\n[STEP 3] Feature Engineering and Daily Aggregation...\")\n",
        "\n",
        "# Advanced Feature: Volume-Weighted PnL (VW_PnL)\n",
        "# This metric prioritizes PnL from trades with high conviction (large volume).\n",
        "df_trader['VW_PnL'] = df_trader['Closed PnL'] * df_trader['Size USD']\n",
        "\n",
        "# Aggregate Trader Performance Metrics by Date\n",
        "daily_trader_metrics = df_trader.groupby('Date').agg(\n",
        "    # --- PROFITABILITY ---\n",
        "    Avg_Daily_PnL=('Closed PnL', 'mean'), # Measures average trade profitability\n",
        "    Total_Daily_PnL=('Closed PnL', 'sum'), # Measures total daily profitability\n",
        "    Median_Daily_PnL=('Closed PnL', 'median'), # Measures typical profitability (less sensitive to large outliers)\n",
        "    Avg_VW_PnL=('VW_PnL', 'sum'), # Measures conviction-adjusted profitability (Sum of PnL * Volume)\n",
        "\n",
        "    # --- RISK & VOLATILITY ---\n",
        "    StdDev_Daily_PnL=('Closed PnL', 'std'), # Measures volatility of trade outcomes (a key risk proxy)\n",
        "    Avg_Daily_Leverage=(LEVERAGE_COL, 'mean'), # Leverage proxy\n",
        "\n",
        "    # --- VOLUME & ACTIVITY ---\n",
        "    Total_Daily_Volume=('Size USD', 'sum'),\n",
        "    Avg_Daily_Volume=('Size USD', 'mean'),\n",
        "    Trade_Count=('Account', 'size')\n",
        ").reset_index()\n",
        "\n",
        "# Handle potential NaN from single-trade days in StdDev\n",
        "daily_trader_metrics['StdDev_Daily_PnL'].fillna(0, inplace=True)\n",
        "\n",
        "print(f\"  > Aggregated metrics created: {len(daily_trader_metrics)} days.\")\n",
        "\n",
        "\n",
        "# --- 4. MERGE DATASETS ---\n",
        "print(\"\\n[STEP 4] Merging Trader Metrics with Market Sentiment...\")\n",
        "\n",
        "df_merged = pd.merge(daily_trader_metrics, df_sentiment, on='Date', how='inner')\n",
        "\n",
        "print(f\"  > Final Merged Dataset size: {len(df_merged)} records (days with both data sources).\")\n",
        "print(f\"  > Data range: {df_merged['Date'].min()} to {df_merged['Date'].max()}\")\n",
        "\n",
        "# --- 5. SAVE INTERMEDIATE OUTPUT ---\n",
        "print(\"\\n[STEP 5] Saving Intermediate Output...\")\n",
        "\n",
        "df_merged.to_csv(OUTPUT_FILE_PATH, index=False)\n",
        "print(f\"SUCCESS: Intermediate merged data saved to: {OUTPUT_FILE_PATH}\")\n",
        "print(\"--- Notebook 1 Completed. Proceed to Notebook 2 for Analysis. ---\")"
      ]
    }
  ]
}